{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import accuracy_score\ndata = pd.read_csv('Salary.csv')\ndata.head()\ndata['Salary'] = data['Salary'].str.replace(',', '').astype(int)\nX = data['Year of Experience'].values\n\ny = data['Salary'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndef LinearRegression(X_train,y_train):\n    n=0\n    d=0 \n    m = 0\n    c = 0\n    for i in range (X_train.shape[0]):\n        \n        n = n+((X_train[i]-X_train.mean())*(y_train[i]-y_train.mean()))\n        d = d+((X_train[i]-X_train.mean())**2)\n    \n    m = n/d\n    c = y_train.mean()-(m*X_train.mean())\n    \n    return m,c\n\ndef predict(m,c,X_test):\n    y=m*X_test+c\n    return y\nm,c=LinearRegression(X_train,y_train)\nprint(m)\nprint(c)\nprint(predict(m,c,X_test[1]))\nprint(X_test[1])\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\ndata = pd.read_csv('PatientDetails_Classification.csv')\n#data = pd.read_csv('diabetes.csv')\ndata=data.drop('Patient Name',axis=1)\nX = data.drop(columns='TARGET').values\n#X = data.drop(columns='Outcome').values\ny = data['TARGET'].values\n#y = data['Outcome'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=56)\n\nclass LogisticRegression:\n\n    def __init__(self, lr=0.001, n_iters=10000):\n        self.lr = lr\n        self.n_iters = n_iters\n        self.w = None\n        self.b = None\n        \n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.w = np.zeros(n_features)\n        self.b = 0\n\n        for _ in range(self.n_iters):\n            linear_pred = np.dot(X, self.w) + self.b\n            predictions = self.sigmoid(linear_pred)\n\n            dw = (1/n_samples) * np.dot(X.T, (predictions - y))\n            db = (1/n_samples) * np.sum(predictions - y)\n\n            self.w = self.w - self.lr * dw\n            self.b = self.b - self.lr * db\n            \n            \n    def predict(self, X):\n        linear_pred = np.dot(X, self.w) + self.b\n        y_pred = self.sigmoid(linear_pred)\n        class_pred = [1 if y > 0.5 else 0 for y in y_pred]\n        return class_pred\n\n\nscaler=MinMaxScaler()\nX_train_scaled=scaler.fit_transform(X_train)\nX_test_scaled=scaler.transform(X_test)\nmodel = LogisticRegression(lr=0.001, n_iters=10000)\nmodel.fit(X_train_scaled, y_train)\npredictions = model.predict(X_test_scaled)\n\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Logistic Regression Accuracy:\", accuracy)\n\ncm=confusion_matrix(y_test,predictions)\ntp=cm[0][0]\nfp=cm[0][1]\nfn=cm[1][0]\ntn=cm[1][1]\nprint(tp)\nprint(fp)\nprint(fn)\nprint(tn)\n\naccuracy=(tp+tn)/(tp+fp+tn+fn)\nprint(accuracy)\n\n\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\ndata = pd.read_csv('diabetes.csv')\ndata.head()\nX = data.drop(columns='Outcome').values\ny = data['Outcome'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\ndef euclidean_distance(point1, point2):\n    return np.sqrt(np.sum((point1 - point2)**2))\n\ndef knn_predict(X_train, y_train, X_test, k):\n    pred = []\n\n    for test_point in X_test:\n        distances = [euclidean_distance(train_point, test_point) for train_point in X_train]\n        nearest_indices = np.argsort(distances)[:k]\n        nearest_labels = y_train[nearest_indices]\n        \n        # Use majority voting to determine the predicted label\n        mejvoting = Counter(nearest_labels).most_common()[0][0] ##(0,3),(1,2)\n        pred.append(mejvoting)\n    \n    return np.array(pred)\n\n\n# Calculate accuracy\ny_pred=knn_predict(X_train, y_train, X_test, k=7)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy}')\n\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred)\ntp=cm[0][0]\nfp=cm[0][1]\nfn=cm[1][0]\ntn=cm[1][1]\nprint(tp)\nprint(fp)\nprint(fn)\nprint(tn)\naccuracy=(tp+tn)/(tp+fp+tn+fn)\nprint(accuracy)\n\nimport numpy as np\nimport pandas as pd\n\nclass SVM:\n    def __init__(self, learning_rate=0.01,n_iters=1000):\n        self.lr = learning_rate\n        self.n_iters = n_iters\n        self.w = None\n        self.b = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.w = np.zeros(n_features)\n        self.b = 0\n\n        for _ in range(self.n_iters):\n            for idx, x_i in enumerate(X):\n                condition = y[idx] * (np.dot(x_i, self.w) + self.b) >= 1\n                if condition:\n                    self.w -= self.lr * (2 * self.w)\n                else:\n                    self.w -= self.lr * (2*self.w - np.dot(x_i, y[idx]))\n                    self.b -= self.lr * y[idx]\n\n\n    def predict(self, X):\n        prediction = np.dot(X, self.w) + self.b\n        return np.sign(prediction).astype(int)\ndata = pd.read_csv('PatientDetails_Classification.csv')\ndata=data.drop('Patient Name',axis=1)\nX = data.drop(columns='TARGET').values\ny = data['TARGET'].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nclf = SVM()\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\nfrom sklearn.metrics import accuracy_score\naccuracy=accuracy_score(y_test,predictions)\nprint(accuracy)\nfrom sklearn.metrics import confusion_matrix\n\ncm=confusion_matrix(y_test,predictions) \ntp=cm[0][0]\nfp=cm[0][1]\nfn=cm[1][0]\ntn=cm[1][1]\nprint(tp)\nprint(fp)\nprint(fn)\nprint(tn)\naccuracy=(tp+tn)/(tp+fp+tn+fn)\nprint(accuracy)\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\ndata = pd.read_csv(\"play_tennis.csv\")\ndata = data.drop('day',axis=1)\nX = data.drop(columns=['play'])\ny = data['play']\ndata['play'] = data['play'].map({'No': 0, 'Yes': 1})\n\ndata.head(15)\n\ndef entropy(y):\n    value_counts = y.value_counts()\n    probabilities = value_counts / len(y)\n    entropy_value = -np.sum(probabilities * np.log2(probabilities.replace(0, 1)))\n    return entropy_value\nentropy(y)\n\ndef information_gain(y, feature):\n\n    total_entropy = entropy(y)\n    \n    unique_values = feature.unique()\n    weighted_entropies = 0\n\n    for value in unique_values:\n        subset_y = y[feature == value]\n        weighted_entropies += (len(subset_y) / len(y)) * entropy(subset_y)\n\n    return total_entropy - weighted_entropies\n\nfor column in data.columns:\n    if column != 'play':\n        feature = data[column]\n        ig = information_gain(y, feature)\n        print(f\"Feature: {column}, Information Gain: {ig:.4f}\")\n\nclass Node:\n    def __init__(self, feature=None, value=None, entropy=None, information_gain=None, left=None, right=None):\n        self.feature = feature\n        self.value = value\n        self.entropy = entropy\n        self.information_gain = information_gain\n        self.left = left\n        self.right = right\n\ndef build_decision_tree(X, y):\n    if entropy(y) == 0:\n        # If all instances have the same class, create a leaf node\n        return Node(value=y.iloc[0])\n\n    if X.empty:\n        # If no features left, create a leaf node with the majority class\n        return Node(value=y.value_counts().idxmax())\n\n    # Find the best feature to split on\n    best_feature = None\n    max_info_gain = 0\n\n    for feature_name in X.columns:\n        current_info_gain = information_gain(y, X[feature_name])\n        if current_info_gain > max_info_gain:\n            max_info_gain = current_info_gain\n            best_feature = feature_name\n\n    # Create a node with the best feature\n    node = Node(feature=best_feature, entropy=entropy(y), information_gain=max_info_gain, value={})\n\n    # Recursively build the left and right subtrees\n    unique_values = X[best_feature].unique()\n    for value in unique_values:\n        subset_X = X[X[best_feature] == value].drop(columns=[best_feature])\n        subset_y = y[X[best_feature] == value]\n        child_node = build_decision_tree(subset_X, subset_y)\n\n        if node.value is None:\n            node.value = {value: child_node}\n        else:\n            node.value[value] = child_node\n\n    return node\n\ndecision_tree = build_decision_tree(X, y)\n\n\ndef predict(node, instance):\n    if node.feature is None:\n        return node.value\n    else:\n        value = instance[node.feature]\n        if value in node.value:\n            return predict(node.value[value], instance)\n        else:\n            return node.value\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ndecision_tree = build_decision_tree(X_train, y_train)\ny_pred = [predict(decision_tree, instance) for _, instance in X_test.iterrows()]\n\naccuracy_score = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy_score:.2f}\")\n\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred)\ntp=cm[0][0]\nfp=cm[0][1]\nfn=cm[1][0]\ntn=cm[1][1]\nprint (tp,fp,fn,tn)\nacc=(tp+tn)/(tp+tn+fp+fn)\nprint (acc)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n\nclass NaiveBayesClassifier:\n    def __init__(self):\n        self.class_probs = {}\n        self.feature_probs = {}\n\n    def fit(self, X, y):\n        num_samples, num_features = X.shape\n        unique_classes = np.unique(y)\n\n        for c in unique_classes:\n            # Calculate class probabilities\n            self.class_probs[c] = np.sum(y == c) / num_samples\n\n            # Calculate feature probabilities for each class\n            features_given_class = X[y == c]\n            self.feature_probs[c] = np.sum(features_given_class, axis=0) / np.sum(y == c)\n\n    def predict(self, X):\n        predictions = []\n\n        for sample in X:\n            class_scores = {}\n\n            for c, class_prob in self.class_probs.items():\n                # Calculate the probability of the sample belonging to each class\n                feature_probs_given_class = self.feature_probs[c]\n                log_prob = np.sum(np.log(sample * feature_probs_given_class ))\n                class_scores[c] = np.log(class_prob) + log_prob\n\n            # Predict the class with the highest probability\n            predicted_class = max(class_scores, key=class_scores.get)\n            predictions.append(predicted_class)\n\n        return predictions\n\ndata = pd.read_csv(\"PatientDetails_Classification.csv\")\n\n# Convert categorical features to numerical values using label encoding\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']).columns:\n    data[column] = label_encoder.fit_transform(data[column])\ndata=data.drop('Patient Name',axis=1)\nX = data.drop('TARGET', axis=1).values\ny = data['TARGET'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nnb_classifier = NaiveBayesClassifier()\nnb_classifier.fit(X_train, y_train)\npredictions = nb_classifier.predict(X_test)\n\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Predictions:\", predictions)\nprint(\"Accuracy:\", accuracy)\n\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,predictions)\ntp=cm[0][0]\nfp=cm[0][1]\nfn=cm[1][0]\ntn=cm[1][1]\nprint(tp)\nprint(fp)\nprint(fn)\nprint(tn)\naccuracy=(tp+tn)/(tp+fp+tn+fn)\nprint(accuracy)\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef pca(X, num_components):\n\n    cov_matrix = np.cov(X, rowvar=False)\n\n    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n\n    # Select the top 'num_components' eigenvectors\n    principal_components = eigenvectors[:, :num_components]\n\n    # Project the standardized data onto the principal components\n    transformed_data = np.dot(X, principal_components)\n\n    return transformed_data, eigenvalues, principal_components\n\ndata = pd.read_csv(\"play_tennis.csv\")\n\n# Convert categorical features to numerical values using label encoding\nlabel_encoder = LabelEncoder()\nfor column in data.select_dtypes(include=['object']).columns:\n    data[column] = label_encoder.fit_transform(data[column])\n\nX = data.drop('play', axis=1).values\ny = data['play'].values\n# Apply PCA\nnum_components = 2\ntransformed_data, eigenvalues, principal_components = pca(X, num_components)\n\nprint(\"Original data shape:\", X.shape)\nprint(\"Transformed data shape:\", transformed_data.shape)\nprint(\"Eigenvalues:\", eigenvalues)\nprint(\"Principal components:\")\nprint(principal_components)\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom collections import Counter\n\nclass RandomForest:\n    def __init__(self, n_trees=100, max_depth=10, min_samples_split=2):\n        self.n_trees = n_trees\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.trees = []\n\n    def fit(self, X, y):\n        self.trees = []\n        for _ in range(self.n_trees):\n            tree = DecisionTreeClassifier(max_depth=self.max_depth,min_samples_split=self.min_samples_split)\n            X_sample, y_sample = self.bootstrap_samples(X, y)\n            tree.fit(X_sample, y_sample)\n            self.trees.append(tree)\n\n    def bootstrap_samples(self, X, y):\n    \n        n_samples = X.shape[0]\n        idxs = np.random.choice(n_samples, n_samples, replace=True)\n        X_bootstrapped = X.reset_index(drop=True).iloc[idxs]\n        y_bootstrapped = y.reset_index(drop=True).iloc[idxs]\n        return X_bootstrapped, y_bootstrapped\n\n    \n\n    def most_common_label(self, y):\n        #counter = Counter(y)\n        most_common = Counter(y).most_common(1)[0][0]\n        return most_common\n\n    def predict(self, X):\n        predictions = np.array([tree.predict(X) for tree in self.trees])\n        tree_preds = np.swapaxes(predictions, 0, 1)\n        predictions = np.array([self.most_common_label(pred) for pred in tree_preds])\n        return predictions\n\ndf = pd.read_csv('diabetes.csv')\n\nX = df.drop(columns=['Outcome'])\n\ny = df['Outcome']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=56)\n\nclf = RandomForest()\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred)\ntp=cm[0][0]\nfp=cm[0][1]\nfn=cm[1][0]\ntn=cm[1][1]\nprint (tp,fp,fn,tn)\naccuracy=(tp+tn)/(tp+fp+tn+fn)\nprint(\"accuracy is:\",accuracy)\n\nimport numpy as np\n\nweight_1 = 0\nweight_2 = 0\nbias = 0\nlearning_rate = 1\n\ndef activation(yin):\n    if yin > 0:\n        return 1\n    elif yin == 0:\n        return 0\n    else:\n        return -1\n        \nepochs = 100\n\ninput_data = np.array([[1, 1], [1, -1], [-1, 1], [-1, -1]])\ntargets = np.array([1, -1, -1, -1])\n\nfor epoch in range(epochs):\n    for i in range(len(input_data)):\n        yin = bias + weight_1 * input_data[i][0] + weight_2 * input_data[i][1]\n        y = activation(yin)\n        if y != targets[i]:\n            weight_1 += learning_rate * targets[i] * input_data[i][0]\n            weight_2 += learning_rate * targets[i] * input_data[i][1]\n            bias += learning_rate * targets[i]\n\nprint(\"Final weights (w1, w2):\", weight_1, weight_2)\nprint(\"Final bias:\", bias)\n\ndef predict_output(x1, x2):\n    yin = bias + weight_1 * x1 + weight_2 * x2\n    return activation(yin)\n\n# Test the trained perceptron\ntest_data = np.array([[-1, 1], [-1, -1]])\nfor x1, x2 in test_data:\n    prediction = predict_output(x1, x2)\n    print(\"Input:\", [x1, x2], \"Predicted Output:\", prediction)\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef distance(point1, point2):\n  return np.sqrt(np.sum((point1 - point2) ** 2))\n\ndef kmeans(data, k, max_iterations):\n  centroids = data[np.random.choice(len(data), k, replace=False)]\n  assignments = np.zeros(len(data))\n  for _ in range(max_iterations):\n    for i, point in enumerate(data):\n      distances_to_centroids = [distance(point, centroid) for centroid in centroids]\n      assignments[i] = np.argmin(distances_to_centroids)\n\n    for cluster in range(k):\n      cluster_data = data[assignments == cluster]\n      if len(cluster_data) > 0:  # Avoid division by zero\n        centroids[cluster] = np.mean(cluster_data, axis=0)\n\n  return centroids, assignments\n\ndata = pd.read_csv(\"Iris.csv\")\n\n#features = list(data.columns)[:-1]\n#X = data[features].to_numpy()\nX = data.iloc[:, :-1].values\nk = 3\nmax_iterations = 100\n\ncentroids, assignments = kmeans(X, k, max_iterations)\n\nclustered_data = []\nfor i in range(k):\n  clustered_data.append(X[assignments == i])\n\nfor i in range(k):\n  print(f\"\\nCluster {i+1} data points:\")\n  print(data.iloc[assignments == i])\n\ncolors = ['red', 'green', 'blue'] \nfor i in range(k):\n  plt.scatter(clustered_data[i][:, 0], clustered_data[i][:, 1], c=colors[i], label=f\"Cluster {i+1}\")\n\nplt.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=100, c='black', label='Centroids')\nplt.xlabel(features[0])\nplt.ylabel(features[1])\nplt.title(f\"K-Means Clustering (k={k})\")\nplt.legend()\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "9423.815323030976\n25321.583011776813\n71498.2780946286\n4.9\nLogistic Regression Accuracy: 0.8333333333333334\n0\n1\n0\n5\n0.8333333333333334\nAccuracy: 0.5052083333333334\n54\n31\n21\n41\n0.6462585034013606\n0.875\n0\n1\n0\n7\n0.875\nFeature: outlook, Information Gain: 0.2467\nFeature: temp, Information Gain: 0.0292\nFeature: humidity, Information Gain: 0.1518\nFeature: wind, Information Gain: 0.0481\nAccuracy: 1.00\n1 0 0 2\n1.0\nPredictions: [1, 0, 0, 1, 0, 0]\nAccuracy: 0.3333333333333333\n0\n0\n4\n2\n0.3333333333333333\nOriginal data shape: (14, 5)\nTransformed data shape: (14, 2)\nEigenvalues: [17.72778608  0.75207765  0.56455197  0.25996934  0.16264793]\nPrincipal components:\n[[-0.99333695 -0.05833223]\n [-0.00625889 -0.81081696]\n [ 0.1127664  -0.51332595]\n [-0.02117184  0.268104  ]\n [-0.00883338 -0.06156241]]\nAccuracy: 0.577922077922078\n60 20 10 28\naccuracy is: 0.7457627118644068\nFinal weights (w1, w2): 1 1\nFinal bias: -1\nInput: [-1, 1] Predicted Output: -1\nInput: [-1, -1] Predicted Output: -1\n\nCluster 1 data points:\n    Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n0    1            5.1           3.5            1.4           0.2  Iris-setosa\n1    2            4.9           3.0            1.4           0.2  Iris-setosa\n2    3            4.7           3.2            1.3           0.2  Iris-setosa\n3    4            4.6           3.1            1.5           0.2  Iris-setosa\n4    5            5.0           3.6            1.4           0.2  Iris-setosa\n5    6            5.4           3.9            1.7           0.4  Iris-setosa\n6    7            4.6           3.4            1.4           0.3  Iris-setosa\n7    8            5.0           3.4            1.5           0.2  Iris-setosa\n8    9            4.4           2.9            1.4           0.2  Iris-setosa\n9   10            4.9           3.1            1.5           0.1  Iris-setosa\n10  11            5.4           3.7            1.5           0.2  Iris-setosa\n11  12            4.8           3.4            1.6           0.2  Iris-setosa\n12  13            4.8           3.0            1.4           0.1  Iris-setosa\n13  14            4.3           3.0            1.1           0.1  Iris-setosa\n14  15            5.8           4.0            1.2           0.2  Iris-setosa\n15  16            5.7           4.4            1.5           0.4  Iris-setosa\n16  17            5.4           3.9            1.3           0.4  Iris-setosa\n17  18            5.1           3.5            1.4           0.3  Iris-setosa\n18  19            5.7           3.8            1.7           0.3  Iris-setosa\n19  20            5.1           3.8            1.5           0.3  Iris-setosa\n20  21            5.4           3.4            1.7           0.2  Iris-setosa\n21  22            5.1           3.7            1.5           0.4  Iris-setosa\n22  23            4.6           3.6            1.0           0.2  Iris-setosa\n23  24            5.1           3.3            1.7           0.5  Iris-setosa\n24  25            4.8           3.4            1.9           0.2  Iris-setosa\n25  26            5.0           3.0            1.6           0.2  Iris-setosa\n26  27            5.0           3.4            1.6           0.4  Iris-setosa\n27  28            5.2           3.5            1.5           0.2  Iris-setosa\n28  29            5.2           3.4            1.4           0.2  Iris-setosa\n29  30            4.7           3.2            1.6           0.2  Iris-setosa\n30  31            4.8           3.1            1.6           0.2  Iris-setosa\n31  32            5.4           3.4            1.5           0.4  Iris-setosa\n32  33            5.2           4.1            1.5           0.1  Iris-setosa\n33  34            5.5           4.2            1.4           0.2  Iris-setosa\n34  35            4.9           3.1            1.5           0.1  Iris-setosa\n35  36            5.0           3.2            1.2           0.2  Iris-setosa\n36  37            5.5           3.5            1.3           0.2  Iris-setosa\n37  38            4.9           3.1            1.5           0.1  Iris-setosa\n38  39            4.4           3.0            1.3           0.2  Iris-setosa\n39  40            5.1           3.4            1.5           0.2  Iris-setosa\n40  41            5.0           3.5            1.3           0.3  Iris-setosa\n41  42            4.5           2.3            1.3           0.3  Iris-setosa\n42  43            4.4           3.2            1.3           0.2  Iris-setosa\n43  44            5.0           3.5            1.6           0.6  Iris-setosa\n44  45            5.1           3.8            1.9           0.4  Iris-setosa\n45  46            4.8           3.0            1.4           0.3  Iris-setosa\n46  47            5.1           3.8            1.6           0.2  Iris-setosa\n47  48            4.6           3.2            1.4           0.2  Iris-setosa\n48  49            5.3           3.7            1.5           0.2  Iris-setosa\n\nCluster 2 data points:\n    Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n49  50            5.0           3.3            1.4           0.2   \n50  51            7.0           3.2            4.7           1.4   \n51  52            6.4           3.2            4.5           1.5   \n52  53            6.9           3.1            4.9           1.5   \n53  54            5.5           2.3            4.0           1.3   \n54  55            6.5           2.8            4.6           1.5   \n55  56            5.7           2.8            4.5           1.3   \n56  57            6.3           3.3            4.7           1.6   \n57  58            4.9           2.4            3.3           1.0   \n58  59            6.6           2.9            4.6           1.3   \n59  60            5.2           2.7            3.9           1.4   \n60  61            5.0           2.0            3.5           1.0   \n61  62            5.9           3.0            4.2           1.5   \n62  63            6.0           2.2            4.0           1.0   \n63  64            6.1           2.9            4.7           1.4   \n64  65            5.6           2.9            3.6           1.3   \n65  66            6.7           3.1            4.4           1.4   \n66  67            5.6           3.0            4.5           1.5   \n67  68            5.8           2.7            4.1           1.0   \n68  69            6.2           2.2            4.5           1.5   \n69  70            5.6           2.5            3.9           1.1   \n70  71            5.9           3.2            4.8           1.8   \n71  72            6.1           2.8            4.0           1.3   \n72  73            6.3           2.5            4.9           1.5   \n73  74            6.1           2.8            4.7           1.2   \n74  75            6.4           2.9            4.3           1.3   \n75  76            6.6           3.0            4.4           1.4   \n76  77            6.8           2.8            4.8           1.4   \n77  78            6.7           3.0            5.0           1.7   \n78  79            6.0           2.9            4.5           1.5   \n79  80            5.7           2.6            3.5           1.0   \n80  81            5.5           2.4            3.8           1.1   \n81  82            5.5           2.4            3.7           1.0   \n82  83            5.8           2.7            3.9           1.2   \n83  84            6.0           2.7            5.1           1.6   \n84  85            5.4           3.0            4.5           1.5   \n85  86            6.0           3.4            4.5           1.6   \n86  87            6.7           3.1            4.7           1.5   \n87  88            6.3           2.3            4.4           1.3   \n88  89            5.6           3.0            4.1           1.3   \n89  90            5.5           2.5            4.0           1.3   \n90  91            5.5           2.6            4.4           1.2   \n91  92            6.1           3.0            4.6           1.4   \n92  93            5.8           2.6            4.0           1.2   \n93  94            5.0           2.3            3.3           1.0   \n94  95            5.6           2.7            4.2           1.3   \n95  96            5.7           3.0            4.2           1.2   \n96  97            5.7           2.9            4.2           1.3   \n97  98            6.2           2.9            4.3           1.3   \n98  99            5.1           2.5            3.0           1.1   \n\n            Species  \n49      Iris-setosa  \n50  Iris-versicolor  \n51  Iris-versicolor  \n52  Iris-versicolor  \n53  Iris-versicolor  \n54  Iris-versicolor  \n55  Iris-versicolor  \n56  Iris-versicolor  \n57  Iris-versicolor  \n58  Iris-versicolor  \n59  Iris-versicolor  \n60  Iris-versicolor  \n61  Iris-versicolor  \n62  Iris-versicolor  \n63  Iris-versicolor  \n64  Iris-versicolor  \n65  Iris-versicolor  \n66  Iris-versicolor  \n67  Iris-versicolor  \n68  Iris-versicolor  \n69  Iris-versicolor  \n70  Iris-versicolor  \n71  Iris-versicolor  \n72  Iris-versicolor  \n73  Iris-versicolor  \n74  Iris-versicolor  \n75  Iris-versicolor  \n76  Iris-versicolor  \n77  Iris-versicolor  \n78  Iris-versicolor  \n79  Iris-versicolor  \n80  Iris-versicolor  \n81  Iris-versicolor  \n82  Iris-versicolor  \n83  Iris-versicolor  \n84  Iris-versicolor  \n85  Iris-versicolor  \n86  Iris-versicolor  \n87  Iris-versicolor  \n88  Iris-versicolor  \n89  Iris-versicolor  \n90  Iris-versicolor  \n91  Iris-versicolor  \n92  Iris-versicolor  \n93  Iris-versicolor  \n94  Iris-versicolor  \n95  Iris-versicolor  \n96  Iris-versicolor  \n97  Iris-versicolor  \n98  Iris-versicolor  \n\nCluster 3 data points:\n      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n99   100            5.7           2.8            4.1           1.3   \n100  101            6.3           3.3            6.0           2.5   \n101  102            5.8           2.7            5.1           1.9   \n102  103            7.1           3.0            5.9           2.1   \n103  104            6.3           2.9            5.6           1.8   \n104  105            6.5           3.0            5.8           2.2   \n105  106            7.6           3.0            6.6           2.1   \n106  107            4.9           2.5            4.5           1.7   \n107  108            7.3           2.9            6.3           1.8   \n108  109            6.7           2.5            5.8           1.8   \n109  110            7.2           3.6            6.1           2.5   \n110  111            6.5           3.2            5.1           2.0   \n111  112            6.4           2.7            5.3           1.9   \n112  113            6.8           3.0            5.5           2.1   \n113  114            5.7           2.5            5.0           2.0   \n114  115            5.8           2.8            5.1           2.4   \n115  116            6.4           3.2            5.3           2.3   \n116  117            6.5           3.0            5.5           1.8   \n117  118            7.7           3.8            6.7           2.2   \n118  119            7.7           2.6            6.9           2.3   \n119  120            6.0           2.2            5.0           1.5   \n120  121            6.9           3.2            5.7           2.3   \n121  122            5.6           2.8            4.9           2.0   \n122  123            7.7           2.8            6.7           2.0   \n123  124            6.3           2.7            4.9           1.8   \n124  125            6.7           3.3            5.7           2.1   \n125  126            7.2           3.2            6.0           1.8   \n126  127            6.2           2.8            4.8           1.8   \n127  128            6.1           3.0            4.9           1.8   \n128  129            6.4           2.8            5.6           2.1   \n129  130            7.2           3.0            5.8           1.6   \n130  131            7.4           2.8            6.1           1.9   \n131  132            7.9           3.8            6.4           2.0   \n132  133            6.4           2.8            5.6           2.2   \n133  134            6.3           2.8            5.1           1.5   \n134  135            6.1           2.6            5.6           1.4   \n135  136            7.7           3.0            6.1           2.3   \n136  137            6.3           3.4            5.6           2.4   \n137  138            6.4           3.1            5.5           1.8   \n138  139            6.0           3.0            4.8           1.8   \n139  140            6.9           3.1            5.4           2.1   \n140  141            6.7           3.1            5.6           2.4   \n141  142            6.9           3.1            5.1           2.3   \n142  143            5.8           2.7            5.1           1.9   \n143  144            6.8           3.2            5.9           2.3   \n144  145            6.7           3.3            5.7           2.5   \n145  146            6.7           3.0            5.2           2.3   \n146  147            6.3           2.5            5.0           1.9   \n147  148            6.5           3.0            5.2           2.0   \n148  149            6.2           3.4            5.4           2.3   \n149  150            5.9           3.0            5.1           1.8   \n\n             Species  \n99   Iris-versicolor  \n100   Iris-virginica  \n101   Iris-virginica  \n102   Iris-virginica  \n103   Iris-virginica  \n104   Iris-virginica  \n105   Iris-virginica  \n106   Iris-virginica  \n107   Iris-virginica  \n108   Iris-virginica  \n109   Iris-virginica  \n110   Iris-virginica  \n111   Iris-virginica  \n112   Iris-virginica  \n113   Iris-virginica  \n114   Iris-virginica  \n115   Iris-virginica  \n116   Iris-virginica  \n117   Iris-virginica  \n118   Iris-virginica  \n119   Iris-virginica  \n120   Iris-virginica  \n121   Iris-virginica  \n122   Iris-virginica  \n123   Iris-virginica  \n124   Iris-virginica  \n125   Iris-virginica  \n126   Iris-virginica  \n127   Iris-virginica  \n128   Iris-virginica  \n129   Iris-virginica  \n130   Iris-virginica  \n131   Iris-virginica  \n132   Iris-virginica  \n133   Iris-virginica  \n134   Iris-virginica  \n135   Iris-virginica  \n136   Iris-virginica  \n137   Iris-virginica  \n138   Iris-virginica  \n139   Iris-virginica  \n140   Iris-virginica  \n141   Iris-virginica  \n142   Iris-virginica  \n143   Iris-virginica  \n144   Iris-virginica  \n145   Iris-virginica  \n146   Iris-virginica  \n147   Iris-virginica  \n148   Iris-virginica  \n149   Iris-virginica  \n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 640x480 with 1 Axes>",
            "image/png": "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"
          },
          "metadata": {}
        },
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'features' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 603\u001b[0m\n\u001b[1;32m    600\u001b[0m   plt\u001b[38;5;241m.\u001b[39mscatter(clustered_data[i][:, \u001b[38;5;241m0\u001b[39m], clustered_data[i][:, \u001b[38;5;241m1\u001b[39m], c\u001b[38;5;241m=\u001b[39mcolors[i], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCluster \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    602\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(centroids[:, \u001b[38;5;241m0\u001b[39m], centroids[:, \u001b[38;5;241m1\u001b[39m], marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCentroids\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 603\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[43mfeatures\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    604\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(features[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    605\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK-Means Clustering (k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}